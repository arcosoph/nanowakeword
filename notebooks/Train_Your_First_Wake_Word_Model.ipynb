{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea663305",
   "metadata": {},
   "source": [
    "# üöÄ Train Your First Custom Wake Word with Nanowakeword!\n",
    "\n",
    "Welcome to the official tutorial for **Nanowakeword**! \n",
    "\n",
    "In this notebook, we will guide you through the entire process of training a high-performance, custom wake word model from scratch. You don't need any pre-existing data‚Äîwe will download everything we need and let Nanowakeword's intelligent engine do the heavy lifting.\n",
    "\n",
    "**Our goal:** Go from zero to a ready-to-use wake word model in just a few simple steps. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d0db6",
   "metadata": {},
   "source": [
    "**Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 1: Install Nanowakeword\n",
    "# We install the full [train] package to get all the necessary dependencies.\n",
    "\n",
    "! pip install --no-cache-dir \"nanowakeword[train]==1.3.1\"\n",
    "! pip install piper-tts\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f231676",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Dataset\n",
    "\n",
    "A great model starts with great data. For this tutorial, we will:\n",
    "1.  **Download** open-source noise and Room Impulse Response (RIR) datasets.\n",
    "2.  **Organize** all your project files within a clean, well-structured folder hierarchy for better clarity and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee93a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 2: Download & Prepare the SonicWeave-v1 Dataset\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_REPO_URL = \"https://huggingface.co/datasets/arcosoph/SonicWeave-v1\"\n",
    "DATA_DIR = Path(\"./nanowakeword_data\")\n",
    "\n",
    "# --- Define Final Paths ---\n",
    "noise_dir = DATA_DIR / \"Noise\"\n",
    "rir_dir = DATA_DIR / \"Rir\"\n",
    "positive_dir = DATA_DIR / \"positive_wakeword\"\n",
    "negative_dir = DATA_DIR / \"negative_speech\"\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"The dataset is downloading. This may take a moment...\")\n",
    "\n",
    "# Download only if the dataset folders are not already created.\n",
    "if not noise_dir.exists() or not rir_dir.exists() or not any(noise_dir.iterdir()):\n",
    "    \n",
    "    # Clone the repository to a temporary location\n",
    "    temp_clone_dir = DATA_DIR / \"temp_repo\"\n",
    "    \n",
    "    print(f\"Downloading the Starter Dataset from {DATASET_REPO_URL}...\")\n",
    "    \n",
    "    # --depth 1 only downloads the latest commit, which is much faster      \n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"clone\", \"--depth\", \"1\", DATASET_REPO_URL, str(temp_clone_dir)],\n",
    "            check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL # Hide unnecessary log messages\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: Failed to clone the dataset repository. Please check the URL.\")\n",
    "        print(f\"Git command failed with error: {e}\")\n",
    "    else:\n",
    "        print(\"Organizing dataset files...\")\n",
    "\n",
    "        # Move the noise and rir folders from the cloned repository to the correct location\n",
    "        try:\n",
    "\n",
    "            # Move only required folders\n",
    "            for folder_name in [\"Noise\", \"Rir\"]:\n",
    "                src = temp_clone_dir / folder_name\n",
    "                dst = DATA_DIR / folder_name\n",
    "                if src.exists():\n",
    "                    shutil.move(str(src), str(dst))\n",
    "\n",
    "            # Delete temp_repo, ignore errors if some files are locked\n",
    "            shutil.rmtree(temp_clone_dir, ignore_errors=True)\n",
    "                        \n",
    "            print(\"\\nSonicWeave-v1 Dataset is ready!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: 'noise' or 'rir' folder not found inside the cloned repository.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error organizing files: {e}\")\n",
    "else:\n",
    "    print(\"SonicWeave-v1 Dataset already found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8452c9f1",
   "metadata": {},
   "source": [
    "## Step 3: Configure and Train the Model\n",
    "\n",
    "Now for the fun part! We will create a `config.yaml` file and then run the Nanowakeword training command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76938d22",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# This section is for our README file.\n",
    "\n",
    "# The Magic: Reading from URL or showing a dropdown\n",
    "import ipywidgets as widgets\n",
    "from google.colab import output\n",
    "\n",
    "# Function to get query parameters from the URL\n",
    "def get_query_param(param):\n",
    "    js_code = f\"return new URL(window.location.href).searchParams.get('{param}');\"\n",
    "    result = output.eval_js(js_code)\n",
    "    return result if result else None\n",
    "\n",
    "# Get model_type from URL, or default to 'dnn'\n",
    "url_model_type = get_query_param('model_type')\n",
    "default_model = url_model_type if url_model_type in [\"dnn\", \"lstm\", \"gru\", \"cnn\", \"rnn\"] else \"dnn\"\n",
    "\n",
    "# Create the interactive dropdown widget\n",
    "model_type_dropdown = widgets.Dropdown(\n",
    "    options=['dnn', 'lstm', 'gru', 'cnn', 'rnn'],\n",
    "    value=default_model,\n",
    "    description='Model Type:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Display the dropdown to the user\n",
    "display(model_type_dropdown)\n",
    "\n",
    "# Later, in the config cell, you will use the selected value\n",
    "selected_model_type = model_type_dropdown.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f77be",
   "metadata": {},
   "source": [
    "**Configuration, Generate Wake Word Samples and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 3.1: Create the Configuration File\n",
    "import yaml\n",
    "\n",
    "# Try to safely get model type, fallback to \"dnn\" if dropdown not defined\n",
    "try:\n",
    "    selected_model = model_type_dropdown.value\n",
    "except NameError:\n",
    "    selected_model = \"dnn\"\n",
    "\n",
    "config_dict = {\n",
    "    # Data Paths (pointing to our newly created folders)\n",
    "    \"positive_data_path\": str(positive_dir),\n",
    "    \"negative_data_path\": str(negative_dir),\n",
    "    \"background_paths\": [str(noise_dir)],\n",
    "    \"rir_paths\": [str(rir_dir)],\n",
    "    # Model Output\n",
    "    \"output_dir\": \"./trained_models\",\n",
    "    \"model_name\": \"hey_computer_v1\",\n",
    "    # your wakeword\n",
    "    \"target_phrase\" : [\"arcosoph\"],  \n",
    "    # How many synthetic data sets do you want to generate?\n",
    "    'generate_positive_samples': 900,      # postive\n",
    "    'generate_negative_samples': 2500,      # negative\n",
    "\n",
    "    \"steps\": 20000,\n",
    "    \"layer_size\": 256,\n",
    "    \"n_blocks\": 3,\n",
    "    'generate_clips': True,\n",
    "    'transform_clips': True,\n",
    "    'train_model': True,\n",
    "    'overwrite': True,\n",
    "    'debug_mode': True,\n",
    "    # Model Type\n",
    "    \"model_type\": selected_model, # DNN offers faster training, while LSTM and other architectures provide greater robustness.\n",
    "    \n",
    "    # Checkpointing & Resuming Configuration (Optional)\n",
    "    \"checkpointing\": {\n",
    "        \"enabled\": True,        # Enable saving checkpoints\n",
    "        \"interval_steps\": 500,  # Save every N steps\n",
    "        \"limit\": 3              # Keep only latest N checkpoints\n",
    "    },\n",
    "\n",
    "    # You can provide other parameters if you want...\n",
    "    # 'augmentation_settings': {\n",
    "    #     'BackgroundNoise': 0.75,\n",
    "    #       ...\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Write the config to a YAML file\n",
    "config_path = \"./config.yaml\"\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úÖ Configuration file saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418ee01",
   "metadata": {},
   "source": [
    "**Run Training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ee1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 3.2: Run the Magic Command! üöÄ\n",
    "# This command will do everything: augment data, extract features, and train the model.\n",
    "# It might take some time depending on the hardware (especially on a CPU).\n",
    "\n",
    "from nanowakeword.trainer import train \n",
    "\n",
    "args_list = [\n",
    "    '--config_path', f'{config_path}',\n",
    "]\n",
    "\n",
    "print(\"Starting NanoWakeWord training...\")\n",
    "\n",
    "try:\n",
    "    train(args_list)\n",
    "    print(\"\\n\\nCONGRATULATIONS! (‚úø‚óï‚Äø‚óï‚úø)\")\n",
    "    print(\"Your custom wake word model has been successfully trained!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2689f74",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You have successfully trained your own custom wake word model!\n",
    "\n",
    "You can now download the `.onnx` file from the `trained_models` directory (check the file browser on the left) and use it in your own applications.\n",
    "\n",
    "For more advanced topics, such as using your own datasets or fine-tuning the configuration, please check out our full documentation on **[GitHub](https://github.com/arcosoph/nanowakeword)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fcd39",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Save Your Model to Google Drive\n",
    "\n",
    "The final step is to save your trained model and performance graph to a safe and accessible place. Instead of a slow direct download, we will save the files directly to your Google Drive. This process is almost instantaneous.\n",
    "\n",
    "Run the cells below to:\n",
    "1.  Connect your Google Drive account.\n",
    "2.  Copy all the trained files into a new folder named `nanowakeword_models` in your Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8dfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 4.1: Connect to Google Drive\n",
    "# This will ask for your permission to access your Google Drive.\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"\\nGoogle Drive connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while connecting to Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 4.2: Copy Final Model and Artifacts to Google Drive üìÇ\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- Configuration ---\n",
    "# Get model_name and output_dir from the config_dict defined earlier\n",
    "model_name = config_dict.get(\"model_name\", \"my_model\")\n",
    "output_dir = config_dict.get(\"output_dir\", \"./trained_models\")\n",
    "\n",
    "# --- Source and Destination Paths ---\n",
    "# The source project directory containing all generated files\n",
    "source_project_dir = os.path.join(output_dir, model_name)\n",
    "\n",
    "# The destination folder in your Google Drive\n",
    "drive_destination_dir = f\"./nanowakeword_models/{model_name}\"\n",
    "\n",
    "# --- Start Copy Process ---\n",
    "print(\"Starting the process to copy trained files to Google Drive...\")\n",
    "\n",
    "# Check if the source directory exists\n",
    "if not os.path.exists(source_project_dir):\n",
    "    print(f\"\\n‚ùå ERROR: Source directory not found at '{source_project_dir}'\")\n",
    "    print(\"This indicates that the training process did not create the expected output folder.\")\n",
    "    print(\"Please ensure the training step completed successfully before running this cell.\")\n",
    "else:\n",
    "    # If an old folder exists in Drive, remove it to ensure a clean copy\n",
    "    if os.path.exists(drive_destination_dir):\n",
    "        print(f\"üîÑ Found an existing folder in Drive. Removing it for a fresh copy: '{drive_destination_dir}'\")\n",
    "        shutil.rmtree(drive_destination_dir)\n",
    "\n",
    "    # --- Copy the entire project folder ---\n",
    "    # This is much simpler and more reliable than copying individual files.\n",
    "    # It preserves the professional directory structure.\n",
    "    try:\n",
    "        shutil.copytree(source_project_dir, drive_destination_dir)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"‚úÖ SUCCESS! All files have been saved to your Google Drive.\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"\\nYour complete project, including the model and performance graphs, can be found in:\")\n",
    "        print(f\"‚û°Ô∏è '{drive_destination_dir}'\")\n",
    "        \n",
    "        # Optional: List the contents of the new folder in Drive for verification\n",
    "        print(\"\\nContents of the saved folder:\")\n",
    "        for root, dirs, files in os.walk(drive_destination_dir):\n",
    "            level = root.replace(drive_destination_dir, '').count(os.sep)\n",
    "            indent = ' ' * 4 * (level)\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            sub_indent = ' ' * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                print(f\"{sub_indent}{f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: An unexpected error occurred during the copy process.\")\n",
    "        print(f\"Details: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
