{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea663305",
   "metadata": {},
   "source": [
    "# üöÄ Train Your First Custom Wake Word with Nanowakeword!\n",
    "\n",
    "Welcome to the official tutorial for **Nanowakeword**! \n",
    "\n",
    "In this notebook, we will guide you through the entire process of training a high-performance, custom wake word model from scratch. You don't need any pre-existing data‚Äîwe will download everything we need and let Nanowakeword's intelligent engine do the heavy lifting.\n",
    "\n",
    "**Our goal:** Go from zero to a ready-to-use wake word model in just a few simple steps. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d0db6",
   "metadata": {},
   "source": [
    "**Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 1: Install Nanowakeword\n",
    "# We install the full [train] package to get all the necessary dependencies.\n",
    "\n",
    "# ! pip install --no-cache-dir \"nanowakeword[train]==1.3.4\"\n",
    "! pip install \"nanowakeword[train] @ git+https://github.com/arcosoph/nanowakeword.git\"\n",
    "! pip install piper-tts\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f231676",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Dataset\n",
    "\n",
    "A great model starts with great data. For this tutorial, we will:\n",
    "1.  **Download** open-source noise and Room Impulse Response (RIR) datasets.\n",
    "2.  **Organize** all your project files within a clean, well-structured folder hierarchy for better clarity and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee93a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 2: Download & Prepare the SonicWeave-v1 Dataset\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_REPO_URL = \"https://huggingface.co/datasets/arcosoph/SonicWeave-v1\"\n",
    "DATA_DIR = Path(\"./nanowakeword_data\")\n",
    "\n",
    "# --- Define Final Paths ---\n",
    "noise_dir = DATA_DIR / \"Noise\"\n",
    "rir_dir = DATA_DIR / \"Rir\"\n",
    "positive_dir = DATA_DIR / \"positive_wakeword\"\n",
    "negative_dir = DATA_DIR / \"negative_speech\"\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"The dataset is downloading. This may take a moment...\")\n",
    "\n",
    "# Download only if the dataset folders are not already created.\n",
    "if not noise_dir.exists() or not rir_dir.exists() or not any(noise_dir.iterdir()):\n",
    "    \n",
    "    # Clone the repository to a temporary location\n",
    "    temp_clone_dir = DATA_DIR / \"temp_repo\"\n",
    "    \n",
    "    print(f\"Downloading the Starter Dataset from {DATASET_REPO_URL}...\")\n",
    "    \n",
    "    # --depth 1 only downloads the latest commit, which is much faster      \n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"clone\", \"--depth\", \"1\", DATASET_REPO_URL, str(temp_clone_dir)],\n",
    "            check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL # Hide unnecessary log messages\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: Failed to clone the dataset repository. Please check the URL.\")\n",
    "        print(f\"Git command failed with error: {e}\")\n",
    "    else:\n",
    "        print(\"Organizing dataset files...\")\n",
    "\n",
    "        # Move the noise and rir folders from the cloned repository to the correct location\n",
    "        try:\n",
    "\n",
    "            # Move only required folders\n",
    "            for folder_name in [\"Noise\", \"Rir\"]:\n",
    "                src = temp_clone_dir / folder_name\n",
    "                dst = DATA_DIR / folder_name\n",
    "                if src.exists():\n",
    "                    shutil.move(str(src), str(dst))\n",
    "\n",
    "            # Delete temp_repo, ignore errors if some files are locked\n",
    "            shutil.rmtree(temp_clone_dir, ignore_errors=True)\n",
    "                        \n",
    "            print(\"\\nSonicWeave-v1 Dataset is ready!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: 'noise' or 'rir' folder not found inside the cloned repository.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error organizing files: {e}\")\n",
    "else:\n",
    "    print(\"SonicWeave-v1 Dataset already found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8452c9f1",
   "metadata": {},
   "source": [
    "## Step 3: Configure and Train the Model\n",
    "\n",
    "Now for the fun part! We will create a `config.yaml` file and then run the Nanowakeword training command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 3.1: Create the \"Beast Mode\" Configuration File\n",
    "import yaml\n",
    "\n",
    "# Define the configuration dictionary\n",
    "config_dict = {\n",
    "    # --- 1. Project Settings ---\n",
    "    \"model_name\": \"user_nww_rnn\",\n",
    "    \"output_dir\": \"./trained_models\",\n",
    "\n",
    "    \n",
    "    # Data Generation Counts\n",
    "    \"generate_positive_samples\": 2044,\n",
    "    \"generate_negative_samples\": 4044, # Should be higher than positive\n",
    "\n",
    "    # TTS Batch Settings (Optimized for Colab/GPU)\n",
    "    \"tts_batch_size\": 256,          # Fast for positive\n",
    "    \"tts_batch_size_negative\": 64,  # Safe for negative (avoids OOM)\n",
    "\n",
    "    # --- 3. Hard Negatives (False Positive Killers) ---\n",
    "    # NOTE: Add words here that sound similar to your target to confuse the model.\n",
    "    # Example: [\"hey arcosoph\", \"hello soph\", \"archive\"]\n",
    "    \"custom_negative_phrases\": [], \n",
    "    \n",
    "    # How many times to repeat each custom phrase?\n",
    "    # \"custom_negative_per_phrase\": 20, # if use custom_nagative_phrases\n",
    "    # \"adversarial_text_generation\": True, # Auto-fill gaps if custom count is low (default: True)\n",
    "\n",
    "    # --- 5. Path Configuration ---\n",
    "    # Using the directory variables defined earlier (recommended) \n",
    "    # or hardcoded strings if you prefer.\n",
    "    \"positive_data_path\": str(positive_dir),\n",
    "    \"negative_data_path\": str(negative_dir),\n",
    "    \"background_paths\": [str(noise_dir)],\n",
    "    \"rir_paths\": [str(rir_dir)],\n",
    "\n",
    "    # --- 6. Augmentation Settings (Per-Example Mode) ---\n",
    "    \"augmentation_rounds\": 10,\n",
    "    \"augmentation_batch_size\": 128,\n",
    "    \"feature_gen_cpu_ratio\": 0.8,\n",
    "    \n",
    "    \"augmentation_settings\": {\n",
    "        \"BackgroundNoise\": 0.8,  # 80% files will have noise\n",
    "        \"RIR\": 0.6,              # 60% files will have reverb\n",
    "        \"PitchShift\": 0.5,\n",
    "        \"Gain\": 1.0,\n",
    "        \"ColoredNoise\": 0.4,\n",
    "        \"BandStopFilter\": 0.2\n",
    "    },\n",
    "\n",
    "    # --- 7. Model Architecture ---\n",
    "    \"model_type\": \"rnn\",     # RNN is excellent for wake words we test it (you can also use others like dnn, transformer...\n",
    "    \"layer_size\": 256,\n",
    "    \"n_blocks\": 4,\n",
    "    \"dropout_prob\": 0.5,\n",
    "    \"embedding_dim\": 64,\n",
    "\n",
    "    # --- 8. Training Hyperparameters ---\n",
    "    \"steps\": 25000,          # 25k steps for robust training\n",
    "    \"batch_size\": 128,\n",
    "    \n",
    "    \"optimizer_type\": \"adamw\",\n",
    "    \"lr_scheduler_type\": \"onecycle\",\n",
    "    \"learning_rate_max\": 0.001,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Loss Weights\n",
    "    \"loss_weight_triplet\": 0.4,\n",
    "    \"loss_weight_class\": 1.0,\n",
    "\n",
    "    # --- 9. Smart Batch Composition ---\n",
    "    \"batch_composition\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"source_distribution\": {\n",
    "            \"positive\": 30,         # 30% Target phrase\n",
    "            \"negative_speech\": 40,  # 40% Human speech (Critical for false positives)\n",
    "            \"pure_noise\": 30        # 30% Background noise\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # --- 10. Checkpointing & Controls ---\n",
    "    \"checkpointing\": {\n",
    "        \"enabled\": True,\n",
    "        \"interval_steps\": 1000,\n",
    "        \"limit\": 5\n",
    "    },\n",
    "    \n",
    "    # --- 11. Pipeline Flags ---\n",
    "    \"overwrite\": True,\n",
    "    \"debug_mode\": True,\n",
    "    \"generate_clips\": True,  # Set to False if you restart training to save time\n",
    "    \"transform_clips\": True,\n",
    "    \"train_model\": True\n",
    "\n",
    "    # You can provide other parameters if you want...\n",
    "}\n",
    "\n",
    "# Write the config to a YAML file\n",
    "config_path = \"./config.yaml\"\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418ee01",
   "metadata": {},
   "source": [
    "**Run Training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ee1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 3.2: Run the Magic Command! üöÄ\n",
    "# This command will do everything: augment data, extract features, and train the model.\n",
    "# It might take some time depending on the hardware (especially on a CPU).\n",
    "\n",
    "from nanowakeword.trainer import train \n",
    "\n",
    "args_list = [\n",
    "    '--config_path', f'{config_path}',\n",
    "]\n",
    "\n",
    "print(\"Starting NanoWakeWord training...\")\n",
    "\n",
    "try:\n",
    "    train(args_list)\n",
    "    print(\"\\n\\nCONGRATULATIONS! (‚úø‚óï‚Äø‚óï‚úø)\")\n",
    "    print(\"Your custom wake word model has been successfully trained!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2689f74",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You have successfully trained your own custom wake word model!\n",
    "\n",
    "You can now download the `.onnx` file from the `trained_models` directory (check the file browser on the left) and use it in your own applications.\n",
    "\n",
    "For more advanced topics, such as using your own datasets or fine-tuning the configuration, please check out our full documentation on **[GitHub](https://github.com/arcosoph/nanowakeword)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fcd39",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Save Your Model to Google Drive\n",
    "\n",
    "The final step is to save your trained model and performance graph to a safe and accessible place. Instead of a slow direct download, we will save the files directly to your Google Drive. This process is almost instantaneous.\n",
    "\n",
    "Run the cells below to:\n",
    "1.  Connect your Google Drive account.\n",
    "2.  Copy all the trained files into a new folder named `nanowakeword_models` in your Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8dfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 4.1: Connect to Google Drive\n",
    "# This will ask for your permission to access your Google Drive.\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"\\nGoogle Drive connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while connecting to Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 4.2: Copy Final Model and Artifacts to Google Drive üìÇ\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- Configuration ---\n",
    "# Get model_name and output_dir from the config_dict defined earlier\n",
    "model_name = config_dict.get(\"model_name\", \"my_model\")\n",
    "output_dir = config_dict.get(\"output_dir\", \"./trained_models\")\n",
    "\n",
    "# --- Source and Destination Paths ---\n",
    "# The source project directory containing all generated files\n",
    "source_project_dir = os.path.join(output_dir, model_name)\n",
    "\n",
    "# The destination folder in your Google Drive\n",
    "drive_destination_dir = f\"drive/MyDrive/nanowakeword_models/{model_name}\"\n",
    "\n",
    "# --- Start Copy Process ---\n",
    "print(\"Starting the process to copy trained files to Google Drive...\")\n",
    "\n",
    "# Check if the source directory exists\n",
    "if not os.path.exists(source_project_dir):\n",
    "    print(f\"\\n‚ùå ERROR: Source directory not found at '{source_project_dir}'\")\n",
    "    print(\"This indicates that the training process did not create the expected output folder.\")\n",
    "    print(\"Please ensure the training step completed successfully before running this cell.\")\n",
    "else:\n",
    "    # If an old folder exists in Drive, remove it to ensure a clean copy\n",
    "    if os.path.exists(drive_destination_dir):\n",
    "        print(f\"üîÑ Found an existing folder in Drive. Removing it for a fresh copy: '{drive_destination_dir}'\")\n",
    "        shutil.rmtree(drive_destination_dir)\n",
    "\n",
    "    # --- Copy the entire project folder ---\n",
    "    # This is much simpler and more reliable than copying individual files.\n",
    "    # It preserves the professional directory structure.\n",
    "    try:\n",
    "        shutil.copytree(source_project_dir, drive_destination_dir)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"‚úÖ SUCCESS! All files have been saved to your Google Drive.\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"\\nYour complete project, including the model and performance graphs, can be found in:\")\n",
    "        print(f\"‚û°Ô∏è '{drive_destination_dir}'\")\n",
    "        \n",
    "        # Optional: List the contents of the new folder in Drive for verification\n",
    "        print(\"\\nContents of the saved folder:\")\n",
    "        for root, dirs, files in os.walk(drive_destination_dir):\n",
    "            level = root.replace(drive_destination_dir, '').count(os.sep)\n",
    "            indent = ' ' * 4 * (level)\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            sub_indent = ' ' * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                print(f\"{sub_indent}{f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: An unexpected error occurred during the copy process.\")\n",
    "        print(f\"Details: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
