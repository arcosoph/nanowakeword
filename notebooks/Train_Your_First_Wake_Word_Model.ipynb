{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea663305",
   "metadata": {},
   "source": [
    "# ðŸš€ Train Your First Custom Wake Word with Nanowakeword!\n",
    "\n",
    "Welcome to the official tutorial for **Nanowakeword**! \n",
    "\n",
    "In this notebook, we will guide you through the entire process of training a high-performance, custom wake word model from scratch. You don't need any pre-existing dataâ€”we will download everything we need and let Nanowakeword's intelligent engine do the heavy lifting.\n",
    "\n",
    "**Our goal:** Go from zero to a ready-to-use wake word model in just a few simple steps. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d0db6",
   "metadata": {},
   "source": [
    "**Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 1: Install Nanowakeword\n",
    "# We install the full [train] package to get all the necessary dependencies.\n",
    "\n",
    "! pip install --no-cache-dir \"nanowakeword[train]==1.2.0\"\n",
    "! pip install piper-tts\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f231676",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Dataset\n",
    "\n",
    "A great model starts with great data. For this tutorial, we will:\n",
    "1.  **Download** open-source noise and Room Impulse Response (RIR) datasets.\n",
    "2.  **Generate** our own custom wake word samples using a built-in TTS engine.\n",
    "3.  **Organize** all your project files within a clean, well-structured folder hierarchy for better clarity and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee93a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is downloading. This may take a moment...\n",
      "Nanowakeword Starter Dataset already found.\n"
     ]
    }
   ],
   "source": [
    "# @title Step 2.1: Download & Prepare the Nanowakeword Starter Dataset\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_REPO_URL = \"https://huggingface.co/datasets/arcosoph/SonicWeave-v1\"\n",
    "DATA_DIR = Path(\"./nanowakeword_data\")\n",
    "\n",
    "# --- Define Final Paths ---\n",
    "noise_dir = DATA_DIR / \"Noise\"\n",
    "rir_dir = DATA_DIR / \"Rir\"\n",
    "positive_dir = DATA_DIR / \"positive_wakeword\"\n",
    "negative_dir = DATA_DIR / \"negative_speech\"\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"The dataset is downloading. This may take a moment...\")\n",
    "\n",
    "# Download only if the dataset folders are not already created.\n",
    "if not noise_dir.exists() or not rir_dir.exists() or not any(noise_dir.iterdir()):\n",
    "    \n",
    "    # Clone the repository to a temporary location\n",
    "    temp_clone_dir = DATA_DIR / \"temp_repo\"\n",
    "    \n",
    "    print(f\"Downloading the Starter Dataset from {DATASET_REPO_URL}...\")\n",
    "    \n",
    "    # --depth 1 only downloads the latest commit, which is much faster      \n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"clone\", \"--depth\", \"1\", DATASET_REPO_URL, str(temp_clone_dir)],\n",
    "            check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL # Hide unnecessary log messages\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: Failed to clone the dataset repository. Please check the URL.\")\n",
    "        print(f\"Git command failed with error: {e}\")\n",
    "    else:\n",
    "        print(\"Organizing dataset files...\")\n",
    "\n",
    "        # Move the noise and rir folders from the cloned repository to the correct location\n",
    "        try:\n",
    "\n",
    "            # Move only required folders\n",
    "            for folder_name in [\"Noise\", \"Rir\"]:\n",
    "                src = temp_clone_dir / folder_name\n",
    "                dst = DATA_DIR / folder_name\n",
    "                if src.exists():\n",
    "                    shutil.move(str(src), str(dst))\n",
    "\n",
    "            # Delete temp_repo, ignore errors if some files are locked\n",
    "            shutil.rmtree(temp_clone_dir, ignore_errors=True)\n",
    "                        \n",
    "            print(\"\\nStarter Dataset is ready!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: 'noise' or 'rir' folder not found inside the cloned repository.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error organizing files: {e}\")\n",
    "else:\n",
    "    print(\"Nanowakeword Starter Dataset already found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cf7c8",
   "metadata": {},
   "source": [
    "**Generate Wake Word Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\t\\.venv\\Lib\\site-packages\\pronouncing\\__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream\n",
      "e:\\t\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\t\\.venv\\Lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  available_backends = torchaudio.list_audio_backends()\n",
      "en_US-ryan-low.onnx: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63.1M/63.1M [00:35<00:00, 1.76MiB/s]\n",
      "en_US-ryan-low.onnx.json: 4.17kiB [00:00, 597kiB/s]\n",
      "en_US-ljspeech-medium.onnx: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63.5M/63.5M [00:36<00:00, 1.75MiB/s]\n",
      "en_US-ljspeech-medium.onnx.json: 4.97kiB [00:00, 4.81MiB/s]\n",
      "en_GB-alan-low.onnx: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63.1M/63.1M [00:34<00:00, 1.82MiB/s]\n",
      "en_GB-alan-low.onnx.json: 4.17kiB [00:00, 1.96MiB/s]\n",
      "Loading voices: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Audio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:03<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 40 intelligent adversarial negative samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Audio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:04<00:00,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All synthetic audio has been generated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Step 2.2: Generate Custom Wake Word & Adversarial Negative Audio\n",
    "\n",
    "from nanowakeword.generate_samples import generate_samples\n",
    "from nanowakeword.data import generate_adversarial_texts \n",
    "\n",
    "#@markdown Define your custom wake word and the number of samples you want to generate.\n",
    "WAKE_WORD = \"Hey Computer\"    #@param {type:\"string\" }\n",
    "NUM_POSITIVE_SAMPLES = 1500   #@param {type:\"integer\"}\n",
    "NUM_NEGATIVE_SAMPLES = 4000   #@param {type:\"integer\"}\n",
    "#    âœï¸(â—”â—¡â—”) à¼¼ ã¤ â—•_â—• à¼½ã¤\n",
    "\n",
    "\n",
    "# 1. Creating positive samples (directly)\n",
    "generate_samples(\n",
    "                text=WAKE_WORD,\n",
    "                output_dir=str(positive_dir),\n",
    "                max_samples=NUM_POSITIVE_SAMPLES\n",
    "          )\n",
    "\n",
    "print(f\"\\nGenerating {NUM_NEGATIVE_SAMPLES} intelligent adversarial negative samples...\")\n",
    "\n",
    "\n",
    "# NanoWakeword will automatically generate strong negative text based on the wakeword.\n",
    "# For example: \"Hey Commuter\", \"Play Computer\", \"Hey Peter\", \"Okay Jupiter\" etc. Thousands of variations\n",
    "adversarial_texts = generate_adversarial_texts(\n",
    "                    input_text=WAKE_WORD,\n",
    "                    N=NUM_NEGATIVE_SAMPLES\n",
    ")\n",
    "\n",
    "# Now create audio from those automatically generated texts\n",
    "generate_samples(\n",
    "                 text=adversarial_texts,\n",
    "                 output_dir=str(negative_dir),\n",
    "                 max_samples=NUM_NEGATIVE_SAMPLES\n",
    ")\n",
    "\n",
    "print(\"\\nAll synthetic audio has been generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d4436",
   "metadata": {},
   "source": [
    "## Step 3: Configure and Train the Model\n",
    "\n",
    "Now for the fun part! We will create a `config.yaml` file and then run the Nanowakeword training command.\n",
    "\n",
    "We will use the magical `--auto-config` flag to let the Intelligent Engine analyze our newly prepared data and build the best possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f77be",
   "metadata": {},
   "source": [
    "**Configuration and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6dd0ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration file saved to ./config.yaml\n"
     ]
    }
   ],
   "source": [
    "# @title Step 3.1: Create the Configuration File\n",
    "import yaml\n",
    "\n",
    "config_dict = {\n",
    "    # Data Paths (pointing to our newly created folders)\n",
    "    \"wakeword_data_path\": str(positive_dir),\n",
    "    \"background_data_path\": str(negative_dir),\n",
    "    \"background_paths\": [str(noise_dir)],\n",
    "    \"rir_paths\": [str(rir_dir)],\n",
    "    # Model Output\n",
    "    \"model_name\": \"hey_computer_v1\",\n",
    "    \"output_dir\": \"./trained_models\",\n",
    "    # Model Type\n",
    "    \"model_type\": \"dnn\" # A good default for many tasks\n",
    "}\n",
    "\n",
    "# Write the config to a YAML file\n",
    "config_path = \"./config.yaml\"\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False)\n",
    "\n",
    "print(f\"âœ… Configuration file saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418ee01",
   "metadata": {},
   "source": [
    "**Run Training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "512ee1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\t\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\t\\.venv\\Lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  available_backends = torchaudio.list_audio_backends()\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NanoWakeWord training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">  _   _               __          __   _     __          __           _ </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> | \\ | |              \\ \\        / /  | |    \\ \\        / /          | |</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> |  \\| | __ _ _ __   __\\ \\  /\\  / /_ _| | ____\\ \\  /\\  / /__  _ __ __| |</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> | . ` |/ _` | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'_ \\ / _ \\ \\/  \\/ / _` | |/ / _ \\ \\/  \\/ / _ \\| '</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">__/ _` |</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> | |\\  | (_| | | | | (_) \\  /\\  / (_| |   &lt;  __/\\  /\\  / (_) | | | (_| |</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> |_| \\_|\\__,_|_| |_|\\___/ \\/  \\/ \\__,_|_|\\_\\___| \\/  \\/ \\___/|_|  \\__,_|</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[1;36m  _   _               __          __   _     __          __           _ \u001b[0m\n",
       "\u001b[1;36m | \\ | |              \\ \\        \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m  | |    \\ \\        \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m          | |\u001b[0m\n",
       "\u001b[1;36m |  \\| | __ _ _ __   __\\ \\  \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m\\  \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m_\u001b[0m\u001b[1;36m _| | ____\\ \\  \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m\\  \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m__\u001b[0m\u001b[1;36m  _ __ __| |\u001b[0m\n",
       "\u001b[1;36m | . ` |\u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m _` | \u001b[0m\u001b[1;36m'_ \\ / _ \\ \\/  \\/ / _` | |/ / _ \\ \\/  \\/ / _ \\| '\u001b[0m\u001b[1;36m__/ _` |\u001b[0m\n",
       "\u001b[1;36m | |\\  | \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m_| | | | | \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m_\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m \\  \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m\\  \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m_| |   <  __/\\  \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m\\  \u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m_\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m | | | \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m_| |\u001b[0m\n",
       "\u001b[1;36m |_| \\_|\\__,_|_| |_|\\___/ \\\u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m  \\\u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m \\__,_|_|\\_\\___| \\\u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m  \\\u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m \\___/|_|  \\__,_|\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">STEP </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">: Verifying and Preprocessing Data Directories</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mSTEP \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m: Verifying and Preprocessing Data Directories\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">====================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "====================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Verifying and preprocessing audio files in: nanowakeword_data\\negative_speech\n",
      "Processing negative_speech: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 57.89it/s]\n",
      "INFO:root:Finished processing directory: nanowakeword_data\\negative_speech. Converted 0 files.\n",
      "INFO:root:Verifying and preprocessing audio files in: nanowakeword_data\\Noise\n",
      "Processing Noise:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1037/1365 [00:21<00:06, 48.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting NanoWakeWord training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCONGRATULATIONS! (âœ¿â—•â€¿â—•âœ¿)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mYour custom wake word model has been successfully trained!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\nanowakeword\\trainer.py:878\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(cli_args)\u001b[39m\n\u001b[32m    875\u001b[39m unique_paths = \u001b[38;5;28mset\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m data_paths_to_process \u001b[38;5;28;01mif\u001b[39;00m p)\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m unique_paths:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m     \u001b[43mverify_and_process_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m print_info(\u001b[33m\"\u001b[39m\u001b[33mData verification and preprocessing complete.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.auto_config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\nanowakeword\\data_utils\\preprocess.py:110\u001b[39m, in \u001b[36mverify_and_process_directory\u001b[39m\u001b[34m(dir_path)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(all_files, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.basename(dir_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.isfile(file_path):\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mprocess_and_convert_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    111\u001b[39m             converted_count += \u001b[32m1\u001b[39m\n\u001b[32m    112\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinished processing directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Converted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverted_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\nanowakeword\\data_utils\\preprocess.py:64\u001b[39m, in \u001b[36mprocess_and_convert_audio\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_and_convert_audio\u001b[39m(file_path):\n\u001b[32m     60\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    Converts a single audio file to the standard format and overwrites it.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    Returns True if a conversion was made, False otherwise.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mneeds_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\nanowakeword\\data_utils\\preprocess.py:42\u001b[39m, in \u001b[36mneeds_conversion\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings():\n\u001b[32m     41\u001b[39m     warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     info = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_path.lower().endswith(\u001b[33m'\u001b[39m\u001b[33m.wav\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\torchaudio\\_internal\\module_utils.py:71\u001b[39m, in \u001b[36mwrap_deprecated.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m     message += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m It will be removed from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33ma future\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mversion\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthe \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m(version)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m release. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     70\u001b[39m warnings.warn(message, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:100\u001b[39m, in \u001b[36mget_info_func.<locals>.info\u001b[39m\u001b[34m(uri, format, buffer_size, backend)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get signal information of an audio file.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03mNote:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m \u001b[33;03m    AudioMetaData\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m backend = dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile.py:15\u001b[39m, in \u001b[36mSoundfileBackend.info\u001b[39m\u001b[34m(uri, format, buffer_size)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfo\u001b[39m(uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike], \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m], buffer_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m4096\u001b[39m) -> AudioMetaData:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoundfile_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:119\u001b[39m, in \u001b[36minfo\u001b[39m\u001b[34m(filepath, format)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;129m@_requires_soundfile\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfo\u001b[39m(filepath: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> AudioMetaData:\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get signal information of an audio file.\u001b[39;00m\n\u001b[32m    103\u001b[39m \n\u001b[32m    104\u001b[39m \u001b[33;03m    Note:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     sinfo = \u001b[43msoundfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AudioMetaData(\n\u001b[32m    121\u001b[39m         sinfo.samplerate,\n\u001b[32m    122\u001b[39m         sinfo.frames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m         encoding=_get_encoding(sinfo.format, sinfo.subtype),\n\u001b[32m    126\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\soundfile.py:488\u001b[39m, in \u001b[36minfo\u001b[39m\u001b[34m(file, verbose)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfo\u001b[39m(file, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    481\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns an object with information about a `SoundFile`.\u001b[39;00m\n\u001b[32m    482\u001b[39m \n\u001b[32m    483\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    486\u001b[39m \u001b[33;03m        Whether to print additional information.\u001b[39;00m\n\u001b[32m    487\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_SoundFileInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\soundfile.py:433\u001b[39m, in \u001b[36m_SoundFileInfo.__init__\u001b[39m\u001b[34m(self, file, verbose)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file, verbose):\n\u001b[32m    432\u001b[39m     \u001b[38;5;28mself\u001b[39m.verbose = verbose\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    434\u001b[39m         \u001b[38;5;28mself\u001b[39m.name = f.name\n\u001b[32m    435\u001b[39m         \u001b[38;5;28mself\u001b[39m.samplerate = f.samplerate\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\soundfile.py:690\u001b[39m, in \u001b[36mSoundFile.__init__\u001b[39m\u001b[34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m._bitrate_mode = bitrate_mode\n\u001b[32m    688\u001b[39m \u001b[38;5;28mself\u001b[39m._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[32m    689\u001b[39m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m \u001b[38;5;28mself\u001b[39m._file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode).issuperset(\u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.seekable():\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[32m    693\u001b[39m     \u001b[38;5;28mself\u001b[39m.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\t\\.venv\\Lib\\site-packages\\soundfile.py:1254\u001b[39m, in \u001b[36mSoundFile._open\u001b[39m\u001b[34m(self, file, mode_int, closefd)\u001b[39m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m             file = file.encode(_sys.getfilesystemencoding())\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     file_ptr = \u001b[43mopenfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m   1256\u001b[39m     file_ptr = _snd.sf_open_fd(file, mode_int, \u001b[38;5;28mself\u001b[39m._info, closefd)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # @title Step 3.2: Run the Magic Command! ðŸš€\n",
    "# # This command will do everything: augment data, extract features, and train the model.\n",
    "# # It might take some time depending on the hardware (especially on a CPU).\n",
    "\n",
    "# # !nanowakeword-train --training_config {config_path} --auto-config --augment_clips  --train_model\n",
    "\n",
    "from nanowakeword.trainer import train \n",
    "\n",
    "args_list = [\n",
    "    '--training_config', f'{config_path}',\n",
    "    '--auto-config',\n",
    "    '--augment_clips',\n",
    "    '--train_model',\n",
    "    '--overwrite' \n",
    "]\n",
    "\n",
    "print(\"Starting NanoWakeWord training...\")\n",
    "\n",
    "try:\n",
    "    train(args_list)\n",
    "    print(\"\\n\\nCONGRATULATIONS! (âœ¿â—•â€¿â—•âœ¿)\")\n",
    "    print(\"Your custom wake word model has been successfully trained!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2689f74",
   "metadata": {},
   "source": [
    "## Step 4: What's Next?\n",
    "\n",
    "You have successfully trained your own custom wake word model!\n",
    "\n",
    "You can now download the `.onnx` or `.tflite` file from the `trained_models` directory (check the file browser on the left) and use it in your own applications.\n",
    "\n",
    "For more advanced topics, such as using your own datasets or fine-tuning the configuration, please check out our full documentation on **[GitHub](https://github.com/arcosoph/nanowakeword)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
