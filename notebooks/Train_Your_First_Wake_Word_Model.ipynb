{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea663305",
   "metadata": {},
   "source": [
    "# üöÄ Train Your First Custom Wake Word with Nanowakeword!\n",
    "\n",
    "Welcome to the official tutorial for **Nanowakeword**! \n",
    "\n",
    "In this notebook, we will guide you through the entire process of training a high-performance, custom wake word model from scratch. You don't need any pre-existing data‚Äîwe will download everything we need and let Nanowakeword's intelligent engine do the heavy lifting.\n",
    "\n",
    "**Our goal:** Go from zero to a ready-to-use wake word model in just a few simple steps. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d0db6",
   "metadata": {},
   "source": [
    "**Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 1: Install Nanowakeword\n",
    "# We install the full [train] package to get all the necessary dependencies.\n",
    "\n",
    "! pip install --no-cache-dir \"nanowakeword[train]==1.2.0\"\n",
    "! pip install piper-tts\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f231676",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Dataset\n",
    "\n",
    "A great model starts with great data. For this tutorial, we will:\n",
    "1.  **Download** open-source noise and Room Impulse Response (RIR) datasets.\n",
    "2.  **Generate** our own custom wake word samples using a built-in TTS engine.\n",
    "3.  **Organize** all your project files within a clean, well-structured folder hierarchy for better clarity and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee93a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 2.1: Download & Prepare the Nanowakeword Starter Dataset\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_REPO_URL = \"https://huggingface.co/datasets/arcosoph/SonicWeave-v1\"\n",
    "DATA_DIR = Path(\"./nanowakeword_data\")\n",
    "\n",
    "# --- Define Final Paths ---\n",
    "noise_dir = DATA_DIR / \"Noise\"\n",
    "rir_dir = DATA_DIR / \"Rir\"\n",
    "positive_dir = DATA_DIR / \"positive_wakeword\"\n",
    "negative_dir = DATA_DIR / \"negative_speech\"\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"The dataset is downloading. This may take a moment...\")\n",
    "\n",
    "# Download only if the dataset folders are not already created.\n",
    "if not noise_dir.exists() or not rir_dir.exists() or not any(noise_dir.iterdir()):\n",
    "    \n",
    "    # Clone the repository to a temporary location\n",
    "    temp_clone_dir = DATA_DIR / \"temp_repo\"\n",
    "    \n",
    "    print(f\"Downloading the Starter Dataset from {DATASET_REPO_URL}...\")\n",
    "    \n",
    "    # --depth 1 only downloads the latest commit, which is much faster      \n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"clone\", \"--depth\", \"1\", DATASET_REPO_URL, str(temp_clone_dir)],\n",
    "            check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL # Hide unnecessary log messages\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: Failed to clone the dataset repository. Please check the URL.\")\n",
    "        print(f\"Git command failed with error: {e}\")\n",
    "    else:\n",
    "        print(\"Organizing dataset files...\")\n",
    "\n",
    "        # Move the noise and rir folders from the cloned repository to the correct location\n",
    "        try:\n",
    "\n",
    "            # Move only required folders\n",
    "            for folder_name in [\"Noise\", \"Rir\"]:\n",
    "                src = temp_clone_dir / folder_name\n",
    "                dst = DATA_DIR / folder_name\n",
    "                if src.exists():\n",
    "                    shutil.move(str(src), str(dst))\n",
    "\n",
    "            # Delete temp_repo, ignore errors if some files are locked\n",
    "            shutil.rmtree(temp_clone_dir, ignore_errors=True)\n",
    "                        \n",
    "            print(\"\\nStarter Dataset is ready!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: 'noise' or 'rir' folder not found inside the cloned repository.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error organizing files: {e}\")\n",
    "else:\n",
    "    print(\"Nanowakeword Starter Dataset already found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cf7c8",
   "metadata": {},
   "source": [
    "**Generate Wake Word Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 2.2: Generate Custom Wake Word & Adversarial Negative Audio\n",
    "\n",
    "# We'll use this code instead of the `--generate_clips` flag.\n",
    "\n",
    "from nanowakeword.generate_samples import generate_samples\n",
    "from nanowakeword.data import generate_adversarial_texts \n",
    "\n",
    "#@markdown Define your custom wake word and the number of samples you want to generate.\n",
    "WAKE_WORD = \"Hey Computer\"    #@param {type:\"string\" }\n",
    "NUM_POSITIVE_SAMPLES = 1000   #@param {type:\"integer\"}\n",
    "NUM_NEGATIVE_SAMPLES = 4000   #@param {type:\"integer\"}\n",
    "#    ‚úçÔ∏è(‚óî‚ó°‚óî) ‡ºº „Å§ ‚óï_‚óï ‡ºΩ„Å§\n",
    "\n",
    "\n",
    "# 1. Creating positive samples (directly)\n",
    "generate_samples(\n",
    "                text=WAKE_WORD,\n",
    "                output_dir=str(positive_dir),\n",
    "                max_samples=NUM_POSITIVE_SAMPLES\n",
    "          )\n",
    "\n",
    "print(f\"\\nGenerating {NUM_NEGATIVE_SAMPLES} intelligent adversarial negative samples...\")\n",
    "\n",
    "# NanoWakeword will automatically generate strong negative text based on the wakeword.\n",
    "# For example: \"Hey Commuter\", \"Play Computer\", \"Hey Peter\", \"Okay Jupiter\" etc. Thousands of variations\n",
    "adversarial_texts = generate_adversarial_texts(\n",
    "                    input_text=WAKE_WORD,\n",
    "                    N=NUM_NEGATIVE_SAMPLES\n",
    ")\n",
    "\n",
    "# Now create audio from those automatically generated texts\n",
    "generate_samples(\n",
    "                 text=adversarial_texts,\n",
    "                 output_dir=str(negative_dir),\n",
    "                 max_samples=NUM_NEGATIVE_SAMPLES\n",
    ")\n",
    "\n",
    "print(\"\\nAll synthetic audio has been generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d4436",
   "metadata": {},
   "source": [
    "## Step 3: Configure and Train the Model\n",
    "\n",
    "Now for the fun part! We will create a `config.yaml` file and then run the Nanowakeword training command.\n",
    "\n",
    "We will use the magical `--auto-config` flag to let the Intelligent Engine analyze our newly prepared data and build the best possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f77be",
   "metadata": {},
   "source": [
    "**Configuration and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 3.1: Create the Configuration File\n",
    "import yaml\n",
    "\n",
    "config_dict = {\n",
    "    # Data Paths (pointing to our newly created folders)\n",
    "    \"wakeword_data_path\": str(positive_dir),\n",
    "    \"background_data_path\": str(negative_dir),\n",
    "    \"background_paths\": [str(noise_dir)],\n",
    "    \"rir_paths\": [str(rir_dir)],\n",
    "    # Model Output\n",
    "    \"model_name\": \"hey_computer_v1\",\n",
    "    \"output_dir\": \"./trained_models\",\n",
    "    # Model Type\n",
    "    \"model_type\": \"dnn\" # DNN offers faster training, while LSTM and other architectures provide greater robustness.\n",
    "}\n",
    "\n",
    "# Write the config to a YAML file\n",
    "config_path = \"./config.yaml\"\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úÖ Configuration file saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418ee01",
   "metadata": {},
   "source": [
    "**Run Training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ee1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 3.2: Run the Magic Command! üöÄ\n",
    "# This command will do everything: augment data, extract features, and train the model.\n",
    "# It might take some time depending on the hardware (especially on a CPU).\n",
    "\n",
    "# The data was generated before, but now we will use the generate_clips flag to adjust the amount of data.\n",
    "\n",
    "from nanowakeword.trainer import train \n",
    "\n",
    "args_list = [\n",
    "    '--training_config', f'{config_path}',\n",
    "    '--auto-config',\n",
    "    '--generate_clips', \n",
    "    '--augment_clips',\n",
    "    '--train_model',\n",
    "    '--overwrite' \n",
    "]\n",
    "\n",
    "print(\"Starting NanoWakeWord training...\")\n",
    "\n",
    "try:\n",
    "    train(args_list)\n",
    "    print(\"\\n\\nCONGRATULATIONS! (‚úø‚óï‚Äø‚óï‚úø)\")\n",
    "    print(\"Your custom wake word model has been successfully trained!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2689f74",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You have successfully trained your own custom wake word model!\n",
    "\n",
    "You can now download the `.onnx` or `.tflite` file from the `trained_models` directory (check the file browser on the left) and use it in your own applications.\n",
    "\n",
    "`If you face any issues while converting to TFLite, don‚Äôt worry. Your ONNX model is entirely your asset, and you can manually convert it to TFLite anytime you wish.`\n",
    "\n",
    "For more advanced topics, such as using your own datasets or fine-tuning the configuration, please check out our full documentation on **[GitHub](https://github.com/arcosoph/nanowakeword)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fcd39",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Save Your Model to Google Drive\n",
    "\n",
    "The final step is to save your trained model and performance graph to a safe and accessible place. Instead of a slow direct download, we will save the files directly to your Google Drive. This process is almost instantaneous.\n",
    "\n",
    "Run the cells below to:\n",
    "1.  Connect your Google Drive account.\n",
    "2.  Copy all the trained files into a new folder named `nanowakeword_models` in your Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8dfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 4.1: Connect to Google Drive\n",
    "# This will ask for your permission to access your Google Drive.\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"\\nGoogle Drive connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while connecting to Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Step 4.2: Copy Trained Files to Your Drive üìÇ (Final Reliable Version)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# Using the configuration of the previous cell\n",
    "model_name = config_dict.get(\"model_name\", \"my_model\")\n",
    "output_dir = config_dict.get(\"output_dir\", \"./trained_models\")\n",
    "\n",
    "# Creating a destination folder in Google Drive\n",
    "drive_folder_path = f\"/content/drive/MyDrive/nanowakeword_models/{model_name}\"\n",
    "\n",
    "# If there is an old folder, delete it and start over.\n",
    "if os.path.exists(drive_folder_path):\n",
    "    print(f\"Removing existing folder in Drive: '{drive_folder_path}'\")\n",
    "    shutil.rmtree(drive_folder_path)\n",
    "\n",
    "os.makedirs(drive_folder_path, exist_ok=True)\n",
    "print(f\"Created a new folder in your Google Drive: '{drive_folder_path}'\")\n",
    "\n",
    "files_copied_count = 0\n",
    "files_found = False\n",
    "\n",
    "# --- 1. Copy the model files (.onnx, .tflite) ---\n",
    "model_files = glob.glob(os.path.join(output_dir, f\"{model_name}*.*\"))\n",
    "for file_path in model_files:\n",
    "    if file_path.endswith(('.onnx', '.tflite')):\n",
    "        try:\n",
    "            shutil.copy(file_path, drive_folder_path)\n",
    "            print(f\"  - Copied model: {os.path.basename(file_path)}\")\n",
    "            files_copied_count += 1\n",
    "            files_found = True\n",
    "        except Exception as e:\n",
    "            print(f\"  - Failed to copy {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "# --- 2. Find and copy the graph folder ---\n",
    "# As of nanowakeword v1.2.0, the graph folder is inside the output_dir, outside the model folder.\n",
    "\n",
    "graphs_source_path_option1 = os.path.join(output_dir, \"graphs\")\n",
    "\n",
    "graphs_source_path_option2 = os.path.join(output_dir, model_name, \"graphs\")\n",
    "\n",
    "graphs_source_path = None\n",
    "if os.path.exists(graphs_source_path_option1):\n",
    "    graphs_source_path = graphs_source_path_option1\n",
    "elif os.path.exists(graphs_source_path_option2):\n",
    "    graphs_source_path = graphs_source_path_option2\n",
    "\n",
    "if graphs_source_path:\n",
    "    graphs_dest_folder = os.path.join(drive_folder_path, \"graphs\")\n",
    "    try:\n",
    "        shutil.copytree(graphs_source_path, graphs_dest_folder)\n",
    "        print(f\"  - Copied performance graphs folder from '{graphs_source_path}'.\")\n",
    "        files_copied_count += 1\n",
    "        files_found = True\n",
    "    except Exception as e:\n",
    "        print(f\"  - Failed to copy graphs folder: {e}\")\n",
    "else:\n",
    "    print(\"  - Performance graphs folder not found.\")\n",
    "\n",
    "if files_found:\n",
    "    print(f\"\\n‚úÖ Success! {files_copied_count} item(s) have been saved to your Google Drive.\")\n",
    "    print(f\"Please check the '{model_name}' folder inside 'nanowakeword_models' in your Google Drive.\")\n",
    "else:\n",
    "    print(f\"Error: Could not find any trained model files or graphs to copy.\")\n",
    "    print(\"Please make sure the training step (3.2) completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
