# ===================================================================
#  NanoWakeWord - Minimal Training Configuration
#  This file contains only the most essential settings to get you started.
#  For a full list of all available options, see `train_config_full.yaml`.⚠️
# ===================================================================


# ⚠️ Important: Fields marked as REQUIRED below must be provided.
# Missing or incorrect values ​​may cause training failure or unexpected results

# ===========================================
# Section 1: Paths & Names
# ===========================================

model_name: my_manual_model_v1 # Give a unique name for your model (default name: user_default_model_v1)
output_dir: ./trained_models    # All outputs (features, models) will be saved here (REQUIRED)

wakeword_data_path: ./training_data/positive #(REQUIRED)
background_data_path: ./training_data/negative #(REQUIRED)

# Extra audio files for data augmentation
background_paths: #(REQUIRED)
  - ./training_data/noise
rir_paths: #(REQUIRED)
  - ./training_data/
  

# =======================================================
# Section 2: Data Generation (only for generate_clips)
# =======================================================

target_phrase: 
  - "jarvis" # For generating synthetic data using TTS

# Number of positive and negative samples to generate for training
generate_positive_samples: 1500  # positive data
generate_negative_samples: 2700 # nagative data

# Batch size for text-to-speech (TTS)
tts_batch_size: 128


# =======================================================
# Section 3: Data Augmentation 
# =======================================================

# How many times each original audio clip will be used during augmentation.
# More rounds = more variation, but also more time.
augmentation_rounds: 7

# Number of clips processed per batch during augmentation.
augmentation_batch_size: 128 


# Background Noise & Reverb Intensity 
# These probabilities (0.0 to 1.0) control how often augmentations are applied.
# Higher values make the model more robust to real-world environments but can
# increase training time and complexity.

# The probability of applying reverberation (echo) to simulate different rooms.
RIR: 0.6

# The probability of mixing in background noise to simulate noisy conditions.
BackgroundNoise: 0.7

# Balances the usage frequency of different background noise folders during augmentation.
# The rates correspond to each path in 'background_paths'. A rate of [3, 1] for two
# paths means clips from the first folder will be used three times as often as the second.
background_paths_duplication_rate:
  - 1

# --- Background Noise Loudness (Signal-to-Noise Ratio in dB) ---
# This range determines how loud the background noise is.

# Minimum SNR. Negative values mean the noise can be LOUDER than the speech.
min_snr_in_db: -5.0

# Maximum SNR. Positive values mean the speech is ALWAYS LOUDER than the noise.
max_snr_in_db: 20.0

# ===========================================
# Section 4: Model Architecture
# ===========================================

# Model type: "dnn", "lstm", "gru", "cnn", "rnn" # (default: DNN)
model_type: gru # Very fast to train (DNN), but LSTM is more powerful. #(default: DNN)

# Size of each model layer (number of neurons).
layer_size: 256

# Total number of layers/blocks in the model.
n_blocks: 3


# =================================================
# Section 5: Training Hyperparameters
# =================================================

# Dropout rate (to reduce overfitting). Range: 0.0 to 1.0
dropout_prob: 0.5

# Type of scheduler to use.
# Options: "cyclic", "onecycle", "cosine".
#   - "cyclic": The powerful default. Bounces LR between a min and max value.
#   - "onecycle": Starts slow, goes up to max, then smoothly down. Great for fast training.
#   - "cosine": Smoothly decreases the LR from max to a minimum value.
lr_scheduler_type: "cyclic"

# Total number of training steps. (And used by 'onecycle' and 'cosine')
steps: 18000

# Maximum learning rate. (Used by all schedulers)
learning_rate_max: 0.0001

# Minimum learning rate. (Used by 'cyclic' and 'cosine')
learning_rate_base: 0.00001

#  Parameters for 'cyclic' scheduler ONLY 
# Number of steps for the "up" phase of CyclicLR.
clr_step_size_up: 3600

# Number of steps for the "down" phase of CyclicLR.
clr_step_size_down: 5400

# =================================================
# Section 6: Batch Composition
# =================================================

batch_composition:
  # Total number of samples per batch during training.
  batch_size: 64

  # Distribution of sample types in each batch (sum must be 100).
  source_distribution:
    positive: 33
    negative_speech: 40
    pure_noise: 27


# Checkpointing & Resuming Configuration (Optional)
checkpointing:
  enabled: true                 # Set to 'true' to enable saving checkpoints. Default is false.
  interval_steps: 500           # Save a checkpoint every N steps.
  limit: 3                      # Keep only the latest N checkpoints to save disk space.


# ==============================================================================
# ⚙️ SECTION X: WORKFLOW STAGES (Set to 'true' or 'false')
# ==============================================================================
# These boolean flags control which major steps of the training pipeline are
# executed. This gives you granular control over the workflow.

# Stage 1: Generate Clips
# Set to 'true' to create new, synthetic audio clips using Text-to-Speech (TTS).
# This is useful for balancing your dataset or creating data from scratch.
generate_clips: true

# Stage 2: Transform Clips
# Set to 'true' to run the core data processing pipeline. This powerful step
# transforms all raw audio clips (both real and synthetic) into the final,

# training-ready numerical features (.npy files) by performing two key actions:
#   1. Data Augmentation: Creates thousands of variations with background noise,
#      reverb, and other effects to make the model robust.
#   2. Feature Extraction: Converts the augmented audio into Mel spectrograms,
#      the numerical format the model understands.
transform_clips: true

# Stage 3: Train Model
# Set to 'true' to run the final model training process using the features
# created in the 'transform_clips' stage.
train_model: true

# oftional: force_verify
# When set to True, forces re-verification of all data directories 
# regardless of previous verification. Normally, verification is automatic and only 
# runs if the dataset has changed or a problem is detected. Use this flag only 
# if you want to manually ensure data integrity. Otherwise, there is no need to provide this parameter.
force_verify: false

# Oftional: overwrite
# When set to True, existing processed features will be regenerated
# if the transform_clips stage is executed. Ensures that all features are
# fresh and consistent with the current configuration and augmentations.
overwrite: True
