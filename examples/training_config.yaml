# ==============================================================================
# Nanowakeword: Advanced Training Configuration Template
#
# This file demonstrates a production-ready configuration for training a robust
# wake word model. It showcases how to fine-tune various aspects of the
# data pipeline, model architecture, and training process.
#
# Any parameter specified here will override the intelligent defaults.
# ==============================================================================

# --- 1. Project Settings ---
model_name: "arcosoph_nww_rnn"
output_dir: "./trained_models"

# --- 2. Target Phrase & Data Generation ---
target_phrase: ["arcosoph"]  # Your wake word

# generate_positive_samples: 50000  # Positive data
# generate_negative_samples: 120000 # Negative data should be much larger (~2.5x)
generate_positive_samples: 2044
generate_negative_samples: 4044

# TTS Batch Settings (Safe and Fast)
tts_batch_size: 256           # For positive samples (Fast)
tts_batch_size_negative: 64   # For negative samples (Safe, avoids memory crash)

# --- 3. Hard Negatives (The False Positive Killers) ---
# Words that sound similar to your wake word, confusing the model.
# We teach these specifically so the model learns the difference.
custom_negative_phrases: [
  "hey arcosoph", "hello arcosoph", 'hi arcosoph', 'he arcosoph',
  'arccopho', 'amrocoph', 'arsokop', 'arrcokoph', '....','......'
]

# How many times these words appear in the generator list? (New Feature)
# Custom nagative words per 
custom_negative_per_phrase: 20

# --- 5. Path Configuration ---
positive_data_path: "./data/generated_positive"
negative_data_path: "./data/generated_negative"
background_paths: ["./data/background_noise"]   # Fan, street noise, etc.
rir_paths: ["./data/rir"]                       # Echo/reverb files

# --- 6. Augmentation Settings (Per-Example Mode Active) ---
# Each audio file gets augmented separately.
augmentation_rounds: 10     # Each file modified 10 different ways
augmentation_batch_size: 128

augmentation_settings:
  BackgroundNoise: 0.8      # 80% of files include noise
  RIR: 0.6                  # 60% of files include echo
  PitchShift: 0.5           # Pitch altered
  Gain: 1.0                 # Volume adjusted
  ColoredNoise: 0.4         # Hissing/mic noise
  BandStopFilter: 0.2       # Some frequencies removed (for bad mics)

feature_gen_cpu_ratio: 0.8

# --- 7. Model Architecture ---
model_type: "rnn"           # DNN or GRU suitable for wake word, DNN is fast and RNN is best and testd
layer_size: 256             # Number of neurons
n_blocks: 4                 # Number of layers (depth)
dropout_prob: 0.5           # 50% dropout to reduce overfitting
embedding_dim: 64           # Feature size

# --- 8. Training Hyperparameters ---
steps: 25000                # 20kâ€“25k steps for Beast Mode
batch_size: 128             # Good for gradient stability

# Optimizer & Scheduler
optimizer_type: "adamw"
lr_scheduler_type: "onecycle" # OneCycleLR used
learning_rate_max: 0.001
weight_decay: 0.01

# Loss Function Control (hardcoded in trainer.py, still good to have)
loss_weight_triplet: 0.4
loss_weight_class: 1.0

# --- 9. Smart Batch Composition ---
# Updated logic in data.py
batch_composition:
  batch_size: 128
  source_distribution:
    positive: 30         # 30% Hello Moka
    negative_speech: 40  # 40% human speech (Hey Moku, Hello etc.)
    pure_noise: 30       # 30% fan/noise

# --- 10. Checkpointing & Early Stopping ---
checkpointing:
  enabled: True
  interval_steps: 1000
  limit: 5

# Patience: how long to wait without improvement before stopping (default: true, According to training steps)
# early_stopping_patience: 0 # or null for trun off 

# --- 11. Advanced ---
overwrite: true 
debug_mode: true          
generate_clips: true # Keep True first time, set False after data generation
transform_clips: true
train_model: true

