# ==========================================================
#       NanoWakeWord Training Configuration
# ==========================================================
# This file controls the entire training pipeline.
# Parameters are grouped logically for clarity and ease of use.
# ==========================================================

## Project & Data Paths
# Define the project name and the locations of all data assets.
model_name: "arcosoph_A_v1"
output_dir: "./trained_models"

# Source audio data paths (raw .wav files)
positive_data_path: "./data/generated_positive"
negative_data_path: "./data/generated_negative"
background_paths: ["./data/background_noise"]
rir_paths: ["./data/rir"]

# Model Architecture
# Configure the structure and properties of the neural network.
model_type: "dnn"
layer_size: 128
n_blocks: 3
dropout_prob: 0.2     # To reduce overfitting

## Training & Optimizer Settings
# Hyperparameters that control the training loop and optimization process.
steps: 20000
batch_size: 128
optimizer_type: "adamw"
learning_rate_max: 0.001
lr_scheduler_type: "onecycle"
weight_decay: 0.01
momentum: 0.9
num_workers: 3

## Feature Manifest
# Paths to the generated .npy feature files. The Acoustic Compositor uses these keys.
# You can add unlimited additional negative sources with unique keys
feature_manifest:
  targets:
    t: './trained_models/arcosoph_A_v1/features/positive_features_train.npy'
  negatives:
    n: "./trained_models/arcosoph_A_v1/features/hard_negative_features.npy"
    # my_other_negatives: "/path/to/other_negatives.npy"
  backgrounds:
    b: './trained_models/arcosoph_A_v1/features/pure_noise_features.npy'
    # For example, adding a large external noise dataset:
    # openwakeword_noise: './openwakeword_features_ACAV100M_2000_hrs_16bit.npy'

## Synthetic Data Generation
# Parameters for generating audio clips using Text-to-Speech (TTS).
generate_positive_samples: 10
generate_negative_samples: 10

include_input_words: 0.7
include_partial_phrase: 0.5

augmentation_batch_size: 16
feature_gen_cpu_ratio: 1.0

feature_generation_manifest:

  positive_wakeword:
    input_audio_dirs: ["./data/generated_positive"]
    output_filename: "positive_features_train.npy"
    use_background_noise: true
    use_rir: true
    
    augmentation_rounds: 11
    # augmentation_settings: (we will use default)

  hard_negatives:
    input_audio_dirs: ["./data/generated_negative"]
    output_filename: "hard_negative_features.npy"
    use_background_noise: true
    use_rir: true

    augmentation_rounds: 11 
  
  pure_ambient_noise:
    input_audio_dirs: ["./data/background_noise"]
    output_filename: "pure_noise_features.npy"
    use_background_noise: false

    augmentation_rounds: 3
    augmentation_settings: 
        PitchShift: 0.6
        AddBackgroundNoise: 0.0
        Gain: 1.0
        RIR: 0.5

## Advanced & Miscellaneous Settings
# Checkpointing, logging, and other fine-tuning parameters.
checkpoint_averaging_top_k: 5
checkpointing:
  enabled: True
  interval_steps: 1000
  limit: 2

early_stopping_patience: 0
main_delta: 0.0001
WARMUP_STEPS: 1500

onnx_opset_version: 17
show_training_summary: true
debug_mode: true
ema_alpha: 0.01

## Pipeline Control Switches
# Master switches to enable or disable major stages of the pipeline.
generate_clips: true
transform_clips: true # Don't forget to turn it off after finished.
train_model: true
overwrite: true
