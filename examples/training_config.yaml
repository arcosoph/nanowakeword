# ==============================================================================
# Nanowakeword: Advanced Training Configuration Template
#
# This file demonstrates a production-ready configuration for training a robust
# wake word model. It showcases how to fine-tune various aspects of the
# data pipeline, model architecture, and training process.
#
# Any parameter specified here will override the intelligent defaults.
# ==============================================================================

# -----------------------------------------------------------------
# 1. Model & Architecture Definition
# -----------------------------------------------------------------
# We select LSTM for its excellent noise robustness.
model_type: "lstm"
model_name: "jarvis_lstm_v1"
output_dir: "./trained_models"

# --- Architecture-specific parameters for 'lstm' ---
# Number of recurrent layers in the LSTM model.
n_blocks: 3
# Number of hidden units in each LSTM layer.
layer_size: 256
# The final dimension of the output embedding vector.
embedding_dim: 128
# Activation function used in some architectures.
activation_function: "relu"


# -----------------------------------------------------------------
# 2. Data Sources & Synthetic Generation
# -----------------------------------------------------------------
positive_data_path: "./training_data/positive"
negative_data_path: "./training_data/negative"
background_paths:
  - "./training_data/noise/ambient"
  - "./training_data/noise/speech_noise"
rir_paths:
  - "./training_data/rir"

# --- TTS Generation ---
target_phrase: ["jarvis"]
generate_positive_samples: 1000
generate_negative_samples: 3000
# Add custom phrases to the negative set to handle specific false-positive cases.
# custom_negative_phrases: ["iss", "just vis", "jaa"] # Optional because Nanowakeword does this very well on its own.


# -----------------------------------------------------------------
# 3. Augmentation & Feature Engineering
# -----------------------------------------------------------------
# Manually override the augmentation policy for a more aggressive strategy.
augmentation_settings:
  BackgroundNoise: 0.85  # Increase probability of adding background noise.
  RIR: 0.70              # Increase probability of adding reverberation.
  PitchShift: 0.40

# Define a custom Signal-to-Noise Ratio (SNR) range for noise augmentation.
min_snr_in_db: -5
max_snr_in_db: 15

# Force regeneration of feature files, overwriting any existing ones.
# Use with caution.
# overwrite: true


# -----------------------------------------------------------------
# 4. Training, Optimization & Loss Function
# -----------------------------------------------------------------
# Manually set the total number of training steps.
steps: 25000

# --- Advanced Batch Composition ---
# Define the exact makeup of each training batch.
batch_composition:
  batch_size: 128
  source_distribution:
    positive: 35
    negative_speech: 45
    pure_noise: 20

# --- Optimizer & Scheduler ---
optimizer_type: "adamw"
learning_rate_max: 0.0002
lr_scheduler_type: "cyclic"


# -----------------------------------------------------------------
# 5. Pipeline Control, Fault Tolerance & Export
# -----------------------------------------------------------------
# Define which stages of the pipeline to execute.
generate_clips: true
transform_clips: true
train_model: true

# --- Fault Tolerance & Checkpointing ---
# Enable and configure the automatic checkpointing system.
checkpointing:
  enabled: true
  interval_steps: 1500
  limit: 5

# --- Export Settings ---
# Specify the ONNX opset version for maximum compatibility.
onnx_opset_version: 17

# --- Debugging ---
# Enable debug mode to save detailed logs.
# debug_mode: true

# and other if you want
